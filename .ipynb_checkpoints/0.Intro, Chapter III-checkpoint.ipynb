{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction ##\n",
    "\n",
    "### Supervised Learning ###\n",
    "Supervised learning is the direct imitation of a pattern between two datasets.\n",
    "Given a dataset as input and another as output, the computer modifies its 'internal procedure' to transform the input dataset to the output dataset. <br>\n",
    "\n",
    "Supervised learning is one of the most popular forms. The following examples all use supervised learning:\n",
    "-  Using the __pixels (input)__ of an image  to detect the presence or absence of a **cat (output) **.\n",
    "-  Using the __liked movies (input)__ to predict ** movies you may like **.\n",
    "-  Using __someone's words (input)__ to predict ** happines or sadness**.<br>\n",
    "\n",
    "__ In general, supervised learning transforms one dataset (what we know) into another dataset (what we want to know). __ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning ###\n",
    "Supervised learning is the direct imitation of a pattern between two datasets.\n",
    "Given a dataset as input and another as output, the computer modifies its 'internal procedure' to transform the input dataset to the output dataset. <br>\n",
    "\n",
    "Supervised learning is one of the most popular forms. The following examples all use supervised learning:\n",
    "-  Using the __pixels (input)__ of an image  to detect the presence or absence of a **cat (output) **.\n",
    "-  Using the __liked movies (input)__ to predict ** movies you may like **.\n",
    "-  Using __someone's words (input)__ to predict ** happines or sadness**.<br>\n",
    "\n",
    "__ In general, supervised learning transforms one dataset (what we know) into another dataset (what we want to know). __ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning ### \n",
    "\n",
    "Unsupervised learning also transforms one input dataset into another. However, the main difference is that the dataset that it transforms into __is not previously known or understood__. <br>\n",
    "Unsupervised learning finds patterns in the data that we don't know about. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Forward  Propagation ##\n",
    "\n",
    "The procedure followed in (supervised) machine learning is __ Predict -> Compare -> Learn __. We will first look at the __\"Predict\"__ part. <br>\n",
    "\n",
    "Note that although the first neural network that we will build shortly, only deals with one datapoint at a time, however this need not be the case. Neural nets can handle multiple datapoints simulataneously, and one question we should always try to answer is __\"how many datapoints should I propagate at a time?\"__. <br>\n",
    "\n",
    "The answer is, that enough datapoints should be passed so that the network can be accurate. For example, a network won't be able\n",
    "to correctly classify whether a photo contains a cat or not, if it is passed one pixel at a time. The general rule of thumb is to provide as much information as a human would need to make the same prediction.\n",
    "\n",
    "### A simple Neural Net making a prediction###\n",
    "Our first neural net will take one input datapoint and output one prediction. Since we only have one input datapoint and one output, our network will have one weight. The network will try to predict __\"win\" (output) __ of the team based on one datapoint containing the __ average number of toes of the team (input)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8500000000000001\n"
     ]
    }
   ],
   "source": [
    "weight = 0.1\n",
    "\n",
    "# We first define the network\n",
    "def neural_network(input, weight):\n",
    "    prediction = input * weight\n",
    "    return prediction\n",
    "\n",
    "# Give it some (input) data points. This will usually be a value recorded in the real world. \n",
    "number_of_toes = [8.5, 9.5, 10, 9]\n",
    "\n",
    "# Pass one datapoint\n",
    "input = number_of_toes[0]\n",
    "\n",
    "# Predict the win based on the input using the network\n",
    "pred = neural_network(input, weight)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interface of the neural network is quite simple: It accepts an __input__ variable as _information_ and a __weight__ variable as _knowledge_. It combines the two (through multiplication) and outputs a _prediction_. All neural nets work in the same way, regardless of the number of input datapoints and number weights. \n",
    "Another way to think about the weights is as a measure of _sensitivity_ between the input and its prediction. \n",
    "\n",
    "Note that although we have managed to make a prediction, it doesn't mean that this was correct. It is through this \"mistakes\" and __trial & error__ that the network will learn: If sees if it has predicted too high or too low and adjusts its weights accordingly in order to predict more accurately the next time it sees the same input.\n",
    "\n",
    "You might have already noticed, that __#of toes__ is not a really good predictor of win. We can modify our network and give it more information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "\n",
    "# Define the network\n",
    "def neural_network2(input, weights):\n",
    "    pred = w_sum(input, weights)\n",
    "    return pred\n",
    "\n",
    "# Define the function used to multiply the inputs by the weights\n",
    "def w_sum(a, b):\n",
    "    assert(len(a) == len(b))\n",
    "    output = 0\n",
    "    \n",
    "    for i in range(len(a)):\n",
    "        output += (a[i] * b[i])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "weights = [0.1, 0.2, 0]\n",
    "\n",
    "# The dataset represent the current status of the team at the beginning of each game\n",
    "# for the first 4 games in a season\n",
    "#\n",
    "# toes := current number of toes\n",
    "# wlrec := current games won (percent)\n",
    "# nfans := fan count (in millions)\n",
    "\n",
    "toes = [8.5, 9.5, 10, 9]\n",
    "wlrec = [0.65, 0.8, 0.8, 0.9]\n",
    "nfans = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "# input corresponds to the team state at the beginning of the first game of the season\n",
    "input = [toes[0], wlrec[0], nfans[0]]\n",
    "\n",
    "pred = neural_network2(input, weights)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the only thing that changed from our first neural net, is that instead of a single value input and weight, we now have vectors and instead of multiplication, we now use a dot product.\n",
    "Loosely stated, a dot product gives us a _notion of similarity_ between two vectors. Consider the examples:\n",
    "\n",
    "-  a = [0, 1, 0, 1]\n",
    "-  b = [1, 0, 1, 0]\n",
    "-  c = [0, 1, 1, 0]\n",
    "-  d = [.5, 0, .5, 0]\n",
    "-  e = [0, 1, -1, 0]\n",
    "<br><br>\n",
    "Which give the dot products:\n",
    "\n",
    "-  w_sum(a,b) = 0\n",
    "-  w_sum(b,c) = 1\n",
    "-  w_sum(b,d) = 1\n",
    "-  w_sum(c,c) = 2\n",
    "-  w_sum(d,d) = .5\n",
    "-  w_sum(c,e) = 0\n",
    "<br><br>\n",
    "Notice that the heighest weighted sum is between vectors that are exactly identical. In contrast, vectors __a__ & __b__ that have no overlapping elements have a dot product of 0. \n",
    "It seems that one could equate the properties of the __\"dot product\"__ to that of a __\"logical AND\"__. This is evident in the _w_sum(a,b)_. \n",
    "Luckily, neural nets are also able to model partial __ANDing__ (for example _w_sum(c,d)_).\n",
    "\n",
    "Following the same analogy, negative weights tend to imply a __logical NOT__ operator, since any positive weight paired with a negative one will cause the score to decrease. If both are negative, the score increases (two negatives make a positive).\n",
    "\n",
    "We can thus \" (crudely) read our weights in the following way:\n",
    "-  weights = [ 1, 0, 1] => if input[0] OR input[2]\n",
    "-  weights = [ 1, 0, -1] => if input[0] OR NOT input[2] \n",
    "-  weights = [ 0.5, 0, 1] => if BIG input[0] or input[2]\n",
    "\n",
    "So given these intuitions, what does it mean for neural net to make a prediction? Roughly speaking it seems that our network gives a high score to inputs that are more _similar_ to our weights. In our weights __weights = [0.1, 0.2, 0]__ notice that the _nfans_ is completely ignored, while the _wlrec_ is the most sensitive predictor.\n",
    "However, the most dominant force int he high score is the _toes_ because the input combined with the weight is by far the highest. (From the result __0.98__, __0.65__ comes from the _toes_ (8.5 x 0.1) and __0.13__ comes from the _wlrec_ (0.65 x 0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Since we are going to be using vectors, we can use numpy, which contains fast vector/matrix\n",
    "# operations written in C code\n",
    "# Using numpy, we don't need our own w_sum method:\n",
    "\n",
    "# redefine our vectors as numpy arrays:\n",
    "weights = np.array([0.1, 0.2, 0])\n",
    "\n",
    "toes = np.array([8.5, 9.5, 10, 9])\n",
    "wlrec = np.array([0.65, 0.8, 0.8, 0.9])\n",
    "nfans = np.array([1.2, 1.3, 0.5, 1.0])\n",
    "\n",
    "def neural_network2(input, weights):\n",
    "    pred = input.dot(weights)\n",
    "    return pred\n",
    "\n",
    "input = np.array([toes[0], wlrec[0], nfans[0]])\n",
    "\n",
    "pred = neural_network2(input, weights)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting multiple Values ###\n",
    "Instead of predicting only whether the team won or lost, we can also predict whether they are happy/sad AND the %age of the team that is hurt.\n",
    "<img src=\"images/0.predicting_multiple_values.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.195, 0.13, 0.5850000000000001]\n"
     ]
    }
   ],
   "source": [
    "weights = [0.3, 0.2, 0.9] \n",
    "\n",
    "def neural_network(input, weights):\n",
    "    pred = ele_mul(input,weights)\n",
    "    return pred\n",
    "\n",
    "def ele_mul(number,vector):\n",
    "    output = [0,0,0]\n",
    "    assert(len(output) == len(vector))\n",
    " \n",
    "    for i in range(len(vector)):\n",
    "         output[i] = number * vector[i]\n",
    "    return output\n",
    "\n",
    "wlrec = [0.65, 0.8, 0.8, 0.9]\n",
    "input = wlrec[0]\n",
    "pred = neural_network(input, weights)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with multiple Inputs & Outputs\n",
    "Conceptually, each input node is connected to each output node as you can see from the image below.\n",
    "For this to be achieved, the input to the network is a vector, and the weights are now a matrix. The i-th row of the weight matrix correspond to the weights for the i-th neuron in the second (output) layer. \n",
    "<img src=\"images/1.multiple_inputs_outputs_net.PNG\">\n",
    "<br>\n",
    "\n",
    "This is what happens when we pass the first vector as the input:\n",
    "<img src=\"images/2.multiple_inputs_outputs_net_1.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.555, 0.9800000000000001, 0.9650000000000001]\n"
     ]
    }
   ],
   "source": [
    "weights = [ \n",
    "    [0.1, 0.1, -0.3],#hurt?\n",
    "    [0.1, 0.2, 0.0], #win?\n",
    "    [0.0, 1.3, 0.1] ]#sad?\n",
    "\n",
    "def vect_mat_mul(input, matrix):\n",
    "    assert(len(input) == len(matrix))\n",
    "    \n",
    "    output = [0 for z in range(len(input))]\n",
    "    \n",
    "    for i in range(len(input)):\n",
    "            output[i] = w_sum(input, matrix[i])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def w_sum(a, b):\n",
    "    assert(len(a) == len(b))\n",
    "    output = 0\n",
    "    \n",
    "    for i in range(len(a)):\n",
    "        output += (a[i] * b[i])\n",
    "    \n",
    "    return output   \n",
    "\n",
    "def neural_network(input, weights):\n",
    "    pred = vect_mat_mul(input,weights)\n",
    "    return pred\n",
    "\n",
    "toes = [8.5, 9.5, 9.9, 9.0]\n",
    "wlrec = [0.65,0.8, 0.8, 0.9]\n",
    "nfans = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "# input corresponds to every entry\n",
    "# for the first game of the season\n",
    "input = [toes[0],wlrec[0],nfans[0]]\n",
    "\n",
    "pred = neural_network(input,weights)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to visualise the current architecture. The first is to think of it as 3 weights coming out of each input node -> Each column of the weight matrix is the weights for each input node.\n",
    "The second way is to think about it as 3 weights going into each output node. -> Each row of the weight matrix contains the weights for each output node. (Look at the image above).\n",
    "Using the second approach, we can think about the network as three independent dot products between the __same__ input vector and the respective __weights__ of the i-th column of the weight matrix (i = 1,2,3)\n",
    "\n",
    "_Note: For those of you experienced with Linear Algebra, the more formal definition would store/process weights as column vectors\n",
    "instead of row vectors. This will be rectified shortly_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting On Predictions ###\n",
    "As you can see in the image below, there is nothing preventing us from taking the output of one network, and feeding it as an input to another network. \n",
    "Practically this is nothing more than two consecutive vector-matrix multiplications. Below is an image of such an architecture:\n",
    "<img src=\"images/3.predicting_on_predictions.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21350000000000002, 0.14500000000000002, 0.5065]\n"
     ]
    }
   ],
   "source": [
    "# ih : input-to-hidden weight matrix\n",
    "ih_wgt = [ [0.1, 0.2, -0.1],#hid[0]\n",
    "           [-0.1,0.1, 0.9], #hid[1]\n",
    "           [0.1, 0.4, 0.1] ]#hid[2]\n",
    "\n",
    "# hp : hidden-to-prediction weight matrix\n",
    "hp_wgt = [ [0.3, 1.1, -0.3],#hurt?\n",
    "           [0.1, 0.2, 0.0], #win?\n",
    "           [0.0, 1.3, 0.1] ]#sad?\n",
    "\n",
    "weights = [ih_wgt, hp_wgt]\n",
    "\n",
    "toes = [8.5, 9.5, 9.9, 9.0]\n",
    "wlrec = [0.65,0.8, 0.8, 0.9]\n",
    "nfans = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "# input corresponds to every entry\n",
    "# for the first game of the season\n",
    "input = [toes[0],wlrec[0],nfans[0]]\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    hid = vect_mat_mul(input, weights[0])\n",
    "    pred = vect_mat_mul(hid, weights[1])\n",
    "    return pred\n",
    "\n",
    "pred = neural_network(input, weights)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Intro to numpy ###\n",
    "Many of the functions that we have written ourselves (dot product, vector-matrix multiplication, etc.) exist in the numpy package. There is no need to reinvent the wheel - we will use these from now on now that we know what happens under the hood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "[4 5 6 7]\n",
      "[[0 1 2 3]\n",
      " [4 5 6 7]]\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 0.65659491  0.22999505  0.08892333  0.00313538  0.32656931]\n",
      " [ 0.90129919  0.76581604  0.87399633  0.16575859  0.06371743]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([0,1,2,3]) # a 1x4 - vector\n",
    "b = np.array([4,5,6,7]) # another 1x4 vector\n",
    "c = np.array([[0,1,2,3],[4,5,6,7]]) # a 2x4 matrix\n",
    "\n",
    "d = np.zeros((2,3)) # a 2x4 matrix of zeros\n",
    "e = np.random.rand(2,5) # a 2x5 random matrix ( uniform[0,1])\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  5, 12, 21])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can do element-wise multiplication between vectors:\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  4,  9],\n",
       "       [ 0,  5, 12, 21]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can even do element-wise multiplication between a vector and a matrix.\n",
    "# This works if the vector and the matrix have the same number of columns. \n",
    "# In this case, the vector is repeated n-times where n is the number of rows of the matrix\n",
    "a * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(4,)\n",
      "(2, 4)\n",
      "(2, 3)\n",
      "(2, 5)\n"
     ]
    }
   ],
   "source": [
    "# The general rule of thumb is that for anything elementwise  (+,-, *, /) to work, the two \n",
    "# variables must either have the SAME number of columns OR one of the variables must only\n",
    "# have 1 column.\n",
    "\n",
    "e * 2 # This will work because the second variable is a scalar\n",
    "# e * a # This will throw an error because e has 5 columns while a has 4.\n",
    "\n",
    "# It is therefore important when reading 'numpy code' to keep in mind the operators and the \n",
    "# dimensions (shapes) of the variables. All numpy objects have the convenient .shape attribute\n",
    "# that returns the variables shape.\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)\n",
    "print(d.shape)\n",
    "print(e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "[[ 14  38]\n",
      " [ 38 126]]\n",
      "[[ 14  38]\n",
      " [ 38 126]]\n"
     ]
    }
   ],
   "source": [
    "# One of the most confusing functions is the .dot() function of the numpy library.\n",
    "# This is because it behaves differently depending on what its arguments are.\n",
    "# If the arguments are 1-D vectors of equal length, then the dot product is returned:\n",
    "dp = a.dot(b)\n",
    "print(dp)\n",
    "\n",
    "# If (one of) the arguments are matrices, then matrix multiplication is performed. Note \n",
    "# that matrix multiplication rules apply (i.e. the number columns of the 1st matrix must\n",
    "# equal the number of columns of the second)\n",
    "\n",
    "mat = c.dot(c.T)\n",
    "print(mat)\n",
    "\n",
    "# Notice that in the second case .dot returns the same result as the np.matmul() function:\n",
    "mat2 = np.matmul(c, c.T)\n",
    "print(mat2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
