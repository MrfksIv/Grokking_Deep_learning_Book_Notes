{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The streetlight problem###\n",
    "We start this chapter by implementing a network that will try to decipher 'the walk/stop' pattern of traffic lights in an imaginary country. The traffic lights in this country have different colors (although they are still 3) and follow different rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This is our input dataset\n",
    "streetlights = np.array([ [ 1, 0, 1 ],\n",
    " [ 0, 1, 1 ],\n",
    " [ 0, 0, 1 ],\n",
    " [ 1, 1, 1 ],\n",
    " [ 0, 1, 1 ],\n",
    " [ 1, 0, 1 ] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streetlights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the corresponding labels of the above dataset (where 1 indicates 'WALK' and 0 indicates 'STOP')\n",
    "walk_vs_stop = np.array( [[ 0 ],\n",
    " [ 1 ],\n",
    " [ 0 ],\n",
    " [ 1 ],\n",
    " [ 1 ],\n",
    " [ 0 ] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walk_vs_stop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [ 0.34291003], Prediction: 0.5855852058304981\n",
      "Error: [ 0.21946242], Prediction: 0.46846816466439845\n",
      "Error: [ 0.14045595], Prediction: 0.37477453173151876\n",
      "Error: [ 0.08989181], Prediction: 0.299819625385215\n",
      "Error: [ 0.05753076], Prediction: 0.23985570030817205\n",
      "Error: [ 0.03681968], Prediction: 0.19188456024653766\n",
      "Error: [ 0.0235646], Prediction: 0.15350764819723012\n",
      "Error: [ 0.01508134], Prediction: 0.12280611855778409\n",
      "Error: [ 0.00965206], Prediction: 0.09824489484622725\n",
      "Error: [ 0.00617732], Prediction: 0.07859591587698181\n",
      "Error: [ 0.00395348], Prediction: 0.06287673270158543\n",
      "Error: [ 0.00253023], Prediction: 0.050301386161268336\n",
      "Error: [ 0.00161935], Prediction: 0.04024110892901467\n",
      "Error: [ 0.00103638], Prediction: 0.032192887143211724\n",
      "Error: [ 0.00066328], Prediction: 0.02575430971456938\n",
      "Error: [ 0.0004245], Prediction: 0.020603447771655514\n",
      "Error: [ 0.00027168], Prediction: 0.016482758217324422\n",
      "Error: [ 0.00017388], Prediction: 0.013186206573859549\n",
      "Error: [ 0.00011128], Prediction: 0.010548965259087661\n",
      "Error: [  7.12196275e-05], Prediction: 0.00843917220727014\n"
     ]
    }
   ],
   "source": [
    "#implement a network\n",
    "\n",
    "# start by initialising a vector of weights. since the input vector is of length 3 and we want a\n",
    "# single value as output, the weights should also be a vector of length 3. The dot product between\n",
    "# the input and \n",
    "weights = np.random.random(3)\n",
    "alpha = 0.1\n",
    "\n",
    "input = streetlights[0]\n",
    "label = walk_vs_stop[0]\n",
    "\n",
    "for iter in range(20):\n",
    "    pred = input.dot(weights)\n",
    "    error = (pred - label) ** 2\n",
    "    delta = pred - label\n",
    "    \n",
    "    delta_weights = delta * input\n",
    "    weights -= alpha * delta_weights\n",
    "    \n",
    "    print(\"Error: {}, Prediction: {}\".format(error, pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that until now, we have been training neural networks on a single observation. Now, we want to train the network to accurately tell us whether \"it is safe to cross the street\". To achieve this, we train the network on all the streetlights dataset at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.0023595707210800187\n",
      "Prediction: 1.0019965501225523\n",
      "Prediction: 0.016738736938228846\n",
      "Prediction: 0.9848726310305989\n",
      "Prediction: 1.002948840198099\n",
      "Prediction: 0.0027447176448562247\n",
      "Error: [ 0.00053481], Prediction: 0.0027447176448562247, Weights: [-0.01381247  0.98607636  0.01600824] \n",
      "\n",
      "Prediction: 0.0021957741158849797\n",
      "Prediction: 1.001865022982405\n",
      "Prediction: 0.015602164647221538\n",
      "Prediction: 0.985899754268448\n",
      "Prediction: 1.0027518510675124\n",
      "Prediction: 0.002554764569304478\n",
      "Error: [ 0.00046464], Prediction: 0.002554764569304478, Weights: [-0.0128775   0.98702469  0.01492131] \n",
      "\n",
      "Prediction: 0.0020438116554435826\n",
      "Prediction: 1.001741623231535\n",
      "Prediction: 0.014542767703275034\n",
      "Prediction: 0.9868571411128269\n",
      "Prediction: 1.0025675935923353\n",
      "Prediction: 0.002378422649074962\n",
      "Error: [ 0.00040369], Prediction: 0.002378422649074962, Weights: [-0.01200544  0.98790806  0.01390818] \n",
      "\n",
      "Prediction: 0.001902738119259971\n",
      "Prediction: 1.0016259587970346\n",
      "Prediction: 0.013555305505894361\n",
      "Prediction: 0.9877495255968484\n",
      "Prediction: 1.0023953313676686\n",
      "Prediction: 0.002214625808978547\n",
      "Error: [ 0.00035073], Prediction: 0.002214625808978547, Weights: [-0.01119213  0.98873098  0.01296383] \n",
      "\n",
      "Prediction: 0.0017717006471828379\n",
      "Prediction: 1.0015176324485189\n",
      "Prediction: 0.012634893368385205\n",
      "Prediction: 0.9885813205264856\n",
      "Prediction: 1.0022343525166792\n",
      "Prediction: 0.0020624085790908134\n",
      "Error: [ 0.00030471], Prediction: 0.0020624085790908134, Weights: [-0.01043367  0.98949765  0.0120836 ] \n",
      "\n",
      "Prediction: 0.0016499268632726508\n",
      "Prediction: 1.0014162484691072\n",
      "Prediction: 0.011776978336083137\n",
      "Prediction: 0.9893566392493016\n",
      "Prediction: 1.002083973091817\n",
      "Prediction: 0.001920893651057071\n",
      "Error: [ 0.00026474], Prediction: 0.001920893651057071, Weights: [-0.00972641  0.99021196  0.01126313] \n",
      "\n",
      "Prediction: 0.0015367149208456565\n",
      "Prediction: 1.0013214176162635\n",
      "Prediction: 0.010977316649546345\n",
      "Prediction: 0.99007931595356\n",
      "Prediction: 1.0019435392373441\n",
      "Prediction: 0.0017892813956491265\n",
      "Error: [ 0.00023001], Prediction: 0.0017892813956491265, Weights: [-0.00906695  0.99087753  0.01049837] \n",
      "\n",
      "Prediction: 0.0014314251165193012\n",
      "Prediction: 1.0012327607386584\n",
      "Prediction: 0.01023195274041861\n",
      "Prediction: 0.9907529245958159\n",
      "Prediction: 1.0018124283977217\n",
      "Prediction: 0.0016668409863723962\n",
      "Error: [ 0.00019983], Prediction: 0.0016668409863723962, Weights: [-0.00845207  0.99149772  0.00978554] \n",
      "\n",
      "Prediction: 0.0013334727890979177\n",
      "Prediction: 1.0011499113406304\n",
      "Prediction: 0.009537199655412927\n",
      "Prediction: 0.9913807965487653\n",
      "Prediction: 1.0016900497972099\n",
      "Prediction: 0.0015529028421999522\n",
      "Error: [ 0.00017362], Prediction: 0.0015529028421999522, Weights: [-0.00787878  0.99207564  0.0091211 ] \n",
      "\n",
      "Prediction: 0.0012423222737599614\n",
      "Prediction: 1.001072517326172\n",
      "Prediction: 0.008889620811060912\n",
      "Prediction: 0.9919660370551614\n",
      "Prediction: 1.0015758443687992\n",
      "Prediction: 0.0014468521573724694\n",
      "Error: [ 0.00015084], Prediction: 0.0014468521573724694, Weights: [-0.0073443   0.9926142   0.00850179] \n",
      "\n",
      "Prediction: 0.0011574817258979764\n",
      "Prediction: 1.0010002421067123\n",
      "Prediction: 0.008286012988560484\n",
      "Prediction: 0.9925115405680005\n",
      "Prediction: 1.0014692842729136\n",
      "Prediction: 0.0013481233302996372\n",
      "Error: [ 0.00013105], Prediction: 0.0013481233302996372, Weights: [-0.00684602  0.9931161   0.00792452] \n",
      "\n",
      "Prediction: 0.001078498664239709\n",
      "Prediction: 1.000932765218877\n",
      "Prediction: 0.007723390484271382\n",
      "Prediction: 0.9930200050519072\n",
      "Prediction: 1.0013698721162931\n",
      "Prediction: 0.0012561951390661595\n",
      "Error: [ 0.00011386], Prediction: 0.0012561951390661595, Weights: [-0.00638149  0.99358383  0.00738644] \n",
      "\n",
      "Prediction: 0.0010049561112529283\n",
      "Prediction: 1.0008697825680024\n",
      "Prediction: 0.007198970337192057\n",
      "Prediction: 0.9934939453156928\n",
      "Prediction: 1.0012771399575442\n",
      "Prediction: 0.0011705865395899183\n",
      "Error: [  9.89217393e-05], Prediction: 0.0011705865395899183, Weights: [-0.00594844  0.99401975  0.00688491] \n",
      "\n",
      "Prediction: 0.0009364692316719348\n",
      "Prediction: 1.0008110063889093\n",
      "Prediction: 0.006710158560132038\n",
      "Prediction: 0.9939357054414288\n",
      "Prediction: 1.0011906481668285\n",
      "Prediction: 0.0010908529854648016\n",
      "Error: [  8.59442057e-05], Prediction: 0.0010908529854648016, Weights: [-0.00554474  0.99442601  0.00641742] \n",
      "\n",
      "Prediction: 0.0008726823883718413\n",
      "Prediction: 1.000756164996079\n",
      "Prediction: 0.006254537306301541\n",
      "Prediction: 0.9943474703710211\n",
      "Prediction: 1.001109984192029\n",
      "Prediction: 0.0010165831870522953\n",
      "Error: [  7.46691945e-05], Prediction: 0.0010165831870522953, Weights: [-0.00516841  0.99480465  0.00598168] \n",
      "\n",
      "Prediction: 0.0008132665496418355\n",
      "Prediction: 1.0007050023799537\n",
      "Prediction: 0.005829852907701587\n",
      "Prediction: 0.9947312767072092\n",
      "Prediction: 1.001034761271751\n",
      "Prediction: 0.0009473962423310065\n",
      "Error: [  6.48733514e-05], Prediction: 0.0009473962423310065, Weights: [-0.00481761  0.99515755  0.00557552] \n",
      "\n",
      "Prediction: 0.0007579169938648047\n",
      "Prediction: 1.0006572776937812\n",
      "Prediction: 0.0054340047260377135\n",
      "Prediction: 0.9950890227820971\n",
      "Prediction: 1.0009646171260018\n",
      "Prediction: 0.0008829390840903494\n",
      "Error: [  5.63626244e-05], Prediction: 0.0008829390840903494, Weights: [-0.0044906   0.99548645  0.00519695] \n",
      "\n",
      "Prediction: 0.000706351267272279\n",
      "Prediction: 1.0006127646656653\n",
      "Prediction: 0.005065034760921258\n",
      "Prediction: 0.9954224780427701\n",
      "Prediction: 1.000899212647886\n",
      "Prediction: 0.0008228841978165603\n",
      "Error: [  4.89684188e-05], Prediction: 0.0008228841978165603, Weights: [-0.00418577  0.99579301  0.00484407] \n",
      "\n",
      "Prediction: 0.0006583073582532477\n",
      "Prediction: 1.0005712509627018\n",
      "Prediction: 0.004721117963886367\n",
      "Prediction: 0.9957332918002189\n",
      "Prediction: 1.0008382306137291\n",
      "Prediction: 0.0007669275725270966\n",
      "Error: [  4.25442584e-05], Prediction: 0.0007669275725270966, Weights: [-0.00390162  0.99607873  0.00451516] \n",
      "\n",
      "Prediction: 0.0006135420580216772\n",
      "Prediction: 1.0005325375279284\n",
      "Prediction: 0.004400553210255219\n",
      "Prediction: 0.9960230013846865\n",
      "Prediction: 1.0007813744243799\n",
      "Prediction: 0.0007147868532237033\n",
      "Error: [  3.69628829e-05], Prediction: 0.0007147868532237033, Weights: [-0.00363675  0.99634504  0.00420858] \n",
      "\n",
      "Prediction: 0.0005718294825789623\n",
      "Prediction: 1.0004964379059238\n",
      "Prediction: 0.004101754884150417\n",
      "Prediction: 0.9962930397476443\n",
      "Prediction: 1.000728366886795\n",
      "Prediction: 0.0006661996688473938\n",
      "Error: [  3.21137273e-05], Prediction: 0.0006661996688473938, Weights: [-0.00338986  0.99659325  0.00392282] \n",
      "\n",
      "Prediction: 0.0005329597350779152\n",
      "Prediction: 1.0004627775690436\n",
      "Prediction: 0.003823245034994561\n",
      "Prediction: 0.9965447425478987\n",
      "Prediction: 1.0006789490421555\n",
      "Prediction: 0.0006209221138632299\n",
      "Error: [  2.79007319e-05], Prediction: 0.0006209221138632299, Weights: [-0.00315972  0.99682461  0.00365646] \n",
      "\n",
      "Prediction: 0.0004967376910905838\n",
      "Prediction: 1.000431393253229\n",
      "Prediction: 0.0035636460666714103\n",
      "Prediction: 0.9967793547567944\n",
      "Prediction: 1.0006328790445573\n",
      "Prediction: 0.0005787273650678278\n",
      "Error: [  2.42404388e-05], Prediction: 0.0005787273650678278, Weights: [-0.0029452   0.99704025  0.00340819] \n",
      "\n",
      "Prediction: 0.0004629818920542624\n",
      "Prediction: 1.0004021323099337\n",
      "Prediction: 0.0033216739231635224\n",
      "Prediction: 0.9969980368151171\n",
      "Prediction: 1.0005899310926072\n",
      "Prediction: 0.0005394044180495466\n",
      "Error: [  2.10603391e-05], Prediction: 0.0005394044180495466, Weights: [-0.00274525  0.99724124  0.00317677] \n",
      "\n",
      "Prediction: 0.00043152353443963714\n",
      "Prediction: 1.0003748520788367\n",
      "Prediction: 0.0030961317369421516\n",
      "Prediction: 0.9972018703721011\n",
      "Prediction: 1.000549894414955\n",
      "Prediction: 0.0005027569300581226\n",
      "Error: [  1.82974362e-05], Prediction: 0.0005027569300581226, Weights: [-0.00255886  0.99742857  0.00296107] \n",
      "\n",
      "Prediction: 0.00040220554404649805\n",
      "Prediction: 1.0003494192845535\n",
      "Prediction: 0.002885903908676533\n",
      "Prediction: 0.9973918636348805\n",
      "Prediction: 1.0005125723097992\n",
      "Prediction: 0.00046860215795817674\n",
      "Error: [  1.58969982e-05], Prediction: 0.00046860215795817674, Weights: [-0.00238513  0.99760319  0.00276001] \n",
      "\n",
      "Prediction: 0.00037488172636654165\n",
      "Prediction: 1.000325709459407\n",
      "Prediction: 0.002689950588967747\n",
      "Prediction: 0.9975689563548135\n",
      "Prediction: 1.0004777812376662\n",
      "Prediction: 0.0004367699815264574\n",
      "Error: [  1.38114733e-05], Prediction: 0.0004367699815264574, Weights: [-0.00222319  0.99776594  0.0025726 ] \n",
      "\n",
      "Prediction: 0.00034941598522116556\n",
      "Prediction: 1.000303606393458\n",
      "Prediction: 0.00250730253480244\n",
      "Prediction: 0.9977340244753148\n",
      "Prediction: 1.0004453499662234\n",
      "Prediction: 0.00040710200366558986\n",
      "Error: [  1.19995481e-05], Prediction: 0.00040710200366558986, Weights: [-0.00207224  0.99791765  0.00239792] \n",
      "\n",
      "Prediction: 0.0003256816029324717\n",
      "Prediction: 1.0002830016123188\n",
      "Prediction: 0.0023370563152766994\n",
      "Prediction: 0.9978878844641647\n",
      "Prediction: 1.0004151187654944\n",
      "Prediction: 0.000379450720204063\n",
      "Error: [  1.04253291e-05], Prediction: 0.000379450720204063, Weights: [-0.00193154  0.99805904  0.00223511] \n",
      "\n",
      "Prediction: 0.0003035605761632504\n",
      "Prediction: 1.0002637938827588\n",
      "Prediction: 0.002178369842870518\n",
      "Prediction: 0.9980312973517041\n",
      "Prediction: 1.0003869386515791\n",
      "Prediction: 0.00035367875286892916\n",
      "Error: [  9.05763171e-06], Prediction: 0.00035367875286892916, Weights: [-0.0018004   0.99819084  0.00208334] \n",
      "\n",
      "Prediction: 0.00028294300229514315\n",
      "Prediction: 1.0002458887457468\n",
      "Prediction: 0.0020304582081640487\n",
      "Prediction: 0.9981649724948786\n",
      "Prediction: 1.0003606706768056\n",
      "Prediction: 0.00032965813978875764\n",
      "Error: [  7.86936232e-06], Prediction: 0.00032965813978875764, Weights: [-0.00167816  0.99831369  0.00194188] \n",
      "\n",
      "Prediction: 0.0002637265118310061\n",
      "Prediction: 1.0002291980762823\n",
      "Prediction: 0.0018925897973890242\n",
      "Prediction: 0.9982895710857346\n",
      "Prediction: 1.0003361852641401\n",
      "Prediction: 0.0003072696785367433\n",
      "Error: [  6.83698183e-06], Prediction: 0.0003072696785367433, Weights: [-0.00156421  0.99842819  0.00181003] \n",
      "\n",
      "Prediction: 0.0002458157428293945\n",
      "Prediction: 1.0002136396691754\n",
      "Prediction: 0.0017640826736084938\n",
      "Prediction: 0.9984057094217169\n",
      "Prediction: 1.0003133615836362\n",
      "Prediction: 0.0002864023172781259\n",
      "Error: [  5.94003918e-06], Prediction: 0.0002864023172781259, Weights: [-0.00145801  0.99853492  0.00168713] \n",
      "\n",
      "Prediction: 0.00022912185382250065\n",
      "Prediction: 1.0001991368497989\n",
      "Prediction: 0.0016443012036223865\n",
      "Prediction: 0.9985139619539326\n",
      "Prediction: 1.0002920869686904\n",
      "Prediction: 0.000266952590060309\n",
      "Error: [  5.16076631e-06], Prediction: 0.000266952590060309, Weights: [-0.00135901  0.9986344   0.00157257] \n",
      "\n",
      "Prediction: 0.00021356207204824718\n",
      "Prediction: 1.0001856181087414\n",
      "Prediction: 0.0015326529139128484\n",
      "Prediction: 0.9986148641284535\n",
      "Prediction: 1.0002722563699111\n",
      "Prediction: 0.00024882409269136796\n",
      "Error: [  4.48372614e-06], Prediction: 0.00024882409269136796, Weights: [-0.00126673  0.99872713  0.00146579] \n",
      "\n",
      "Prediction: 0.00019905927415309445\n",
      "Prediction: 1.0001730167592444\n",
      "Prediction: 0.001428585560076217\n",
      "Prediction: 0.9987089150347098\n",
      "Prediction: 1.000253771844446\n",
      "Prediction: 0.00023192699600385093\n",
      "Error: [  3.89550676e-06], Prediction: 0.00023192699600385093, Weights: [-0.00118072  0.99881356  0.00136627] \n",
      "\n",
      "Prediction: 0.00018554159680308092\n",
      "Prediction: 1.0001612706162761\n",
      "Prediction: 0.0013315843952447096\n",
      "Prediction: 0.9987965798740667\n",
      "Prediction: 1.0002365420786832\n",
      "Prediction: 0.00021617759360872848\n",
      "Error: [  3.38445580e-06], Prediction: 0.00021617759360872848, Weights: [-0.00110055  0.99889412  0.0012735 ] \n",
      "\n",
      "Prediction: 0.00017294207488698283\n",
      "Prediction: 1.000150321696097\n",
      "Prediction: 0.001241169623985986\n",
      "Prediction: 0.9988782922607928\n",
      "Prediction: 1.0002204819423204\n",
      "Prediction: 0.0002014978815106864\n",
      "Error: [  2.94044954e-06], Prediction: 0.0002014978815106864, Weights: [-0.00102583  0.99896921  0.00118703] \n",
      "\n",
      "Prediction: 0.00016119830520854917\n",
      "Prediction: 1.0001401159351844\n",
      "Prediction: 0.0011568940290857053\n",
      "Prediction: 0.9989544563668017\n",
      "Prediction: 1.0002055120718785\n",
      "Prediction: 0.00018781516719164695\n",
      "Error: [  2.55469240e-06], Prediction: 0.00018781516719164695, Weights: [ -9.56174132e-04   9.99039202e-01   1.10642627e-03] \n",
      "\n",
      "Prediction: 0.00015025213375331758\n",
      "Prediction: 1.0001306029274084\n",
      "Prediction: 0.001078340759473782\n",
      "Prediction: 0.9990254489207674\n",
      "Prediction: 1.000191558481826\n",
      "Prediction: 0.00017506170597836237\n",
      "Error: [  2.21954268e-06], Prediction: 0.00017506170597836237, Weights: [ -8.91250408e-04   9.99104441e-01   1.03129977e-03] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for iter in range(40):\n",
    "    error_for_all_lights = 0\n",
    "    \n",
    "    for row_index in range(len(walk_vs_stop)):\n",
    "        input = streetlights[row_index]\n",
    "        label = walk_vs_stop[row_index]\n",
    "        \n",
    "        pred = input.dot(weights)\n",
    "        error_for_all_lights += (pred - label) ** 2\n",
    "        \n",
    "        delta = pred - label\n",
    "        delta_weights = delta * input\n",
    "        weights -= alpha * delta_weights\n",
    "        print(\"Prediction: {}\".format(pred))\n",
    "    \n",
    "    print(\"Error: {}, Prediction: {}, Weights: {} \\n\".format(error_for_all_lights, pred, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full / Batch / Stochastic Gradient Descent ###\n",
    "__Stochastic gradient descent__ is what we have just done - i.e. when our algorithm tries to learn from the whole dataset __but__ _one example at a time_. I.e. it calculates deltas and updates the weights after each and every forward propagation - once for each observation.<br><br>\n",
    "\n",
    "In __Full Gradient Descent__, instead of updating multiple times, the network estimates the average weight_delta over the entire dataset and updates the weights once.<br><br>\n",
    "\n",
    "__Batch Gradient Descent__, is a combination of the two, where the dataset is broken into several batches. Then, average weight_deltas are calculated for each batch. _N_ updates are performed, where _n_ is the total number of batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Learn Correlation ###\n",
    "Consider the weights after the last iteration above: <br>\n",
    "`Weights: [ -8.91250408e-04   9.99104441e-01   1.03129977e-03] `<br><br>\n",
    "The network has managed to identify that whenever the second input is 'On' -1- the output is 'WALK'. This is because each neuron is individually trying to correctly predict the output given the input. The only communication between neurons comes from the shared error. Our update rule, simply multiplies this shared error by each neuron's input, which attributes error to each neuron:\n",
    "<img src=\"images/10.Error_Attribution.PNG\">\n",
    "\n",
    "When the input is 0, no pressure is exerted on the neuron. When the the input is 1, but the output is 0, -ve pressure is exerted because the term (pred-input)input is negative. When the input is 1, and the output is 1, +ve pressure is exerted. And this is how correlation is achieved: If the input has no effect on the output then due to randomness, about half of the pressures will be positive and half will be -ve. The sum of these will be 0. On the other hand, positive correlation will lead to a positive weight because most of the updates will exert un upward pressure. The opposite of these will hold for negative correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Cases ###\n",
    "It should be noted that the discussion above regarding weights learning correlation oversimplifies a lot of things but it is helpful to understand how the network learns. However, there are edge cases where things don't work out as predicted by our correlation argument.\n",
    "\n",
    "#### Overfitting ####\n",
    "Overfitting is Deep Learning's Greatest Weakness. It happens when a particular configuration of weights _accidentally_ creates perfect correlation between our inputs and the outputs. Because the error is 0, the network stops learning. Although the network has managed (accidentally) to perfectly predict the learning dataset it will fail in the real world with unseen data. <br>\n",
    "\n",
    "__The greatest challenge in deep learning is to make the network _generalise_ instead of just _memorise_.__\n",
    "\n",
    "#### Conflicting Pressure ####\n",
    "In some sense we were lucky because in our previous example, the middle input has perfect correlation with the output. In most of the cases, correlation will not be that clear. Consider the example below:\n",
    "<img src=\"images/11.Unclear_Correlation.PNG\">\n",
    "\n",
    "There exists no correlation between the training data and the output. All of the neurons are equally balanced between +ve and -ve pressure. We will somehow have to make the network learn indirect correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Indirect Correlation ###\n",
    "Until now, although we have used the analogy of networks, we have done little more than multiple linear regresssion (without a bias term).\n",
    "\\begin{equation*}\n",
    "Y_i   = w_1x_{i1} + w_2x_{i2} + w_3x_{i3}\n",
    "\\end{equation*}\n",
    "Where $ Y_i $ is the i-th label of the output dataset and $ x_{ij} $ is the j-th input of the i-th row of the input dataset. Linear regression is unable (at least in this form) to learn indirect correlation. This is where the power of neural networks comes in. We are going to use our input dataset to create an intermediate dataset that DOES have correlation with our output. This will be our new architecture:\n",
    "<img src=\"images/12.Hidden_Layer_Architecture.PNG\">\n",
    "Our goal is to train the above networks so as to create correlation between our hidden layer and the output layer. Note that __gradient descent__ still works since we can calculate the error attribution of each neuron between its input and output layers. <br>\n",
    "If we actually ignore the input layer, then everything we know still holds. We can still attribute error and update the weights between the new hidden layer and the output layer. What we need to do, is find a way to also attribute error and update the weights from the input layer to the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation: Long Distance Error Attribution ###\n",
    "Note that the prediction at layer 2 is simply a weighted sum of the values at layer_1 wight the weights_1_2. Using that prediction we can calculate the delta at layer_2. Then we should somehow break down this layer_2 delta to the contributions of each neuron at layer_1 <br>\n",
    "How do we know how much each neuron at layer_1 contributed to the layer_2 delta? Well, neurons with higher weights_1_2 contributed more. __Our weights from layer_1 to layer_2 exactly describe how much each layer_1 neuron contributed to the layer_2 prediction__. This means that they __ALSO describe how much each neuron contributed to the error (since the error is a function of the prediction)__.<br><br>\n",
    "So to find our delta at layer_1 we simply multiply the delta at layer_2 with the weights_1_2. Basically, we follow our prediction logic in reverse: We multiply the output_delta with its preceding weights to find the delta at the previous layer.\n",
    "\n",
    "<img src=\"images/13.Backpropagating_delta.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation let us say: OK, if you want this output neuron in layer_2 to be X amount higher/lower (where X is the layer_2 delta), then these previous 4 neurons need to be X * weights_1_2 higher/lower because these weights are amplifying the prediction by weights_1_2.<br><br>\n",
    "\n",
    "When used in reverse, weights_1_2 amplify the error so that we know how much each Layer_1 node should move up or down (these are the layer_1 deltas). Once we know that, we can use the same rule as before to update our weights in the previous layer.\n",
    "\n",
    "But before we do that, we should take a detour into non-linearities.\n",
    "\n",
    "### Linear Vs Non-Linear ###\n",
    "The first thing to note is that for any two or more multiplications that we perform, we can achieve the _same result_ using a single multiplication: \n",
    "\\begin{equation*}\n",
    "1 * 10 * 2 = 100 \\equiv 5 * 20 = 100\n",
    "\\end{equation*}\n",
    "Why do we care? Well, what the above effectively means, is that __for any 3-layer network we create, there exists a 2-layer netowrk that has identical behaviour__. In other words, two consecutive weighted sums is just a more  computationally expensive version of a single weighted sum:\n",
    "<img src=\"images/14.Linear_3_Layer_Network.PNG\">\n",
    "The image above, shows clearly that our architecture thus far hasn't been able to add anything new. The middle nodes don't actually serve any purpose so the output of our 3 layer linear network will be the same. They don't have correlation of their own - they are just more or less correlated with the various input nodes. What we really need is for our middle layer to _selectively_ correalte with an input.  <br>\n",
    "We want to give the opportunity to the middle layer to not just \"always be X% correlated to one input and Y% correlated to another input\". Instead, it can be \"X% correlated to one input.... only when it wants to be... but other times not be correlated at all!\" (i.e. to have a conditional correlation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The secret to \"Conditional Correlation\" ###\n",
    "__ We are going to turn the node \"off\" whenever its value is below 0.__ This means that now our node can selectively pick when it wants to be correlated to something. This will allow it to do stuff like \"make me perfectly correlated to the left input __but only if__ the right node is turned OFF. <br>\n",
    "To achieve this, it would give a weight of 1 to the left input and a huge -ve number to the weight of the right input. Whenever the right input is positive (i.e. \"ON\") the weighted sum would be < 0 which will be turned to 0 based on our new condition. If the right value is \"OFF\" then the node would take the value of the left node.\n",
    "<br><br>\n",
    "Note that this wasn't possible before. Before, our middle node was either ALWAYS correlated to an input or ALWAYS uncorrelated. \n",
    "#### Nonlinearity ####\n",
    "The condition that we have applied by setting to zero all values < 0 is called a nonlinearity. In mathematical terms, it prevents the consecutive matrix multiplication from being just a linear transformation. Note that there exist lots of nonlinearityies, with the one we have just discussed being the simplest, called \"relu\".\n",
    "\n",
    "### A small recap ###\n",
    "Before moving on lets summarise in two sentences what we have learned thus far:\n",
    "-  We can compute the relationship between our error and any of our weights so that we know how changing the weight changes the error. Using this, we can reduce our error (almost) to 0.\n",
    "-  Adjusting our weights over a series of training examples, effectively searchs for correlation between our input and output layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our First \"Deep\" Neural Network ###\n",
    "In the following code, we perform a forward propagation through a 3 layer network using a relu activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.634231159844\n",
      "Error: 0.358384076763\n",
      "Error: 0.0830183113303\n",
      "Error: 0.0064670549571\n",
      "Error: 0.000329266900075\n",
      "Error: 1.50556226651e-05\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "alpha = 0.2\n",
    "hidden_size = 4 # Number of nodes of our hidden layer\n",
    "iterations = 60 # Number of iterations to perform over the  entire dataset\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "# The derivative of the relu function is f`(x) = 1 , if  x > 0, and 0 otherwise\n",
    "def relu2deriv(output):\n",
    "    return output > 0\n",
    "\n",
    "streetlights = np.array([\n",
    "     [ 1, 0, 1 ],\n",
    "     [ 0, 1, 1 ],\n",
    "     [ 0, 0, 1 ],\n",
    "     [ 1, 1, 1 ] ] )\n",
    "\n",
    "walk_vs_stop = np.array([[1, 1, 0, 0]]).T \n",
    "\n",
    "weights_0_1 = 2 * np.random.random((3, hidden_size)) - 1\n",
    "weights_1_2 = 2 * np.random.random((hidden_size, 1)) - 1\n",
    "\n",
    "for iter in range(iterations):\n",
    "    layer_2_error = 0\n",
    "    for i in range(len(streetlights)):\n",
    "        layer_0 = streetlights[i: i+1] # (1 x 3)\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1)) # (1 x hidden_size)\n",
    "        layer_2 = layer_1.dot(weights_1_2) # (1 x 1)\n",
    "        \n",
    "        layer_2_error += np.sum((layer_2 - walk_vs_stop[i: i+1]) ** 2)\n",
    "        \n",
    "        layer_2_delta = (layer_2 - walk_vs_stop[i: i+1]) # (1 x 1)\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1) # (1 x hidden_size) ### THIS IS THE ONLY\n",
    "                                                                                                   ### NEW LINE OF CODE! \n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "    \n",
    "    if (iter % 10 ==9) :\n",
    "        print(\"Error: {}\".format(str(layer_2_error)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we multiply by the derivative when calculating the layer_1_delta is because if the output of a layer1 node was 0, it did not contribute at all to the error and by extention, it should play any part in the error attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
